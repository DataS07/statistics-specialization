---
title: "Peer Assessment II"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---

# Background

As a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.

# Training Data and relevant packages

In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.

```{r load, message = FALSE}
load("ames_train.Rdata")
```

Use the code block below to load any necessary packages

```{r packages, message = FALSE}
library(statsr)
library(ggplot2)
library(dplyr)
library(statsr)
library(BAS)
library(GGally)
library("gplots")
library(gridExtra)
library(grid)
library(plotmo)
library(MASS)
library(tidyverse)
library(broom)
library(naniar)
library(visdat)
library("cowplot")
library(tidyverse)
library(caret)


options(width=600)
```

## Part 1 - Exploratory Data Analysis (EDA)

When you first get your data, it's very tempting to immediately begin fitting models and assessing how they perform.  However, before you begin modeling, it's absolutely essential to explore the structure of the data and the relationships between the variables in the data set.

Do a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see. 

After you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).

* * *

### Filtering by Normal Sale Condition

Since we want to compare the price of houses to its sale price under normal condition, we are going to subset the training data:
```{r subset, message = FALSE}
# Backup complete dataset
ames_train_full <- data.frame(ames_train)

# Filter Normal Sale Condition
ames_train %>%
  filter(Sale.Condition == "Normal") -> ames_train

# Remove Sale Condition factor
ames_train <- dplyr::select(ames_train, -Sale.Condition)
```

Other variables are not associated to the houses' value, and are also removed:
```{r}
ames_train <- dplyr::select(ames_train, -Mo.Sold, -Yr.Sold, -Sale.Type, -Misc.Val, -Misc.Feature)

```


### **Figure 1**
**Comparing the distibution of price by neighborhood before and after filtering by Normal Sale Condition**

One important plot we must show is how the price is distributed among neighborhoods. By this plot, we can see the difference of prices in each neighborhood, as well as how the price varies among them.

We are going to use this plot to check how the price distribution among Neighborhood changed after filtering by Normal Sale Condition:

```{r graph1}
# Boxplot before filtering
ggplot(ames_train_full, aes(x=reorder(Neighborhood, -price, FUN = median), y=price)) + geom_boxplot()  + labs(title = "(A) All Sale Condition", caption = "Ordered by Median.") + xlab("Neighborhood") + ylab("Price") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Boxplot after filtering
ggplot(ames_train, aes(x=reorder(Neighborhood, -price, FUN = median), y=price)) + geom_boxplot()  + labs(title = "(B) Normal Sale Condition", caption = "Ordered by Median.") + xlab("Neighborhood") + ylab("Price") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

We can see that the **order of the neighborhoods with the largest prices changed**. For instance, in Figure 1 (B) we see that StoneBr is now the top-4 most expensive neighborhood, while in Figure 1 (A) it is the top-1. **The variance of some neighborhoods also changed**. For instance, before the filter (A), the variance of StoneBr was similar to the one found in NridgHt; after the filter (B), we see that the boxplot is shorter. 

Given that the filter altered the order and variance of neighborhoods' price, **the above comparison makes it even more important to consider only the Normal Sale Condition** in our analysis, once our purpose is to helping to assess whether the asking price of a house is higher or lower than the true value of the house.

### **Figure 2**    
**Understanding variables types and discarting NAs**

It is important to understand the type of our data and to analyse the number of Missing Values (NAs). In this figure, we visualize in (A) the main category and in (B) the variables missing values, ordered by percentage.

```{r graph2}

vis_dat(ames_train) + theme(axis.text.x = element_text(angle = 90, size = 6))+ labs(title = "(A) Data types", caption = "Ordered by Type.")

vis_miss(ames_train, sort_miss = TRUE) + theme(axis.text.x = element_text(angle = 90, size = 6))+ labs(title = "(B) Missing Values", caption = "Ordered by number of missing values.")


```


In Figure 2 (A), we can see that half of our data is categorical and half is numerical. In Figure 2 (B), we can see that there are 3 to 5 variables with a large number of missing values. **We chose to discart the variables Pool.QC, Alley and Fence  because they have more than 50% of missing values each**. Also, the fact that a house has pool or not is already described by the Pool.Area.
```{r}
# Removing variables that has more than 50% of NAs
lowNAs <- lapply( ames_train, function(x) sum(is.na(x)) / length(x) ) < 0.5
ames_train <- ames_train[lowNAs]
```

The variable **Fireplace.Qu** was also removed, due to large number of NAs and because there is another variable that indicates the number of Fireplaces that does not contain NAs.
```{r}
# Removing Fireplace.Qu
ames_train <- dplyr::select(ames_train, -Fireplace.Qu)
```

The variable **Lot.Frontage** also contains many NAs. This variable relates to the **Linear feet of street connected to property** and the minimun value is 21. Therefore, we will set all NAs to 0, supposing they filled with NAs when there was no connection between the street connected to property.  

```{r}
# Setting Lot.Frontage NAs to Zero
summary(ames_train$Lot.Frontage)
ames_train$Lot.Frontage[is.na(ames_train$Lot.Frontage)] <- 0
ames_train$Lot.Frontage <- as.integer(ames_train$Lot.Frontage)
summary(ames_train$Lot.Frontage)
```

Some of the variables related to Garage and Basement (variables that start with the words Garage and Bsmt) were also removed. We kept their versions without NAs. For instance, **we kept Garage.Cars and Garage.Area**. Similar for Basement.

```{r}
ames_train <- dplyr::select(ames_train, -Garage.Qual, -Garage.Cond, -Garage.Type, -Garage.Yr.Blt, -Garage.Finish, -Bsmt.Cond, -Bsmt.Qual, -Bsmt.Exposure, -BsmtFin.Type.1, -BsmtFin.Type.2, -BsmtFin.SF.2)
```

Finally, we have only the variable **Mas.Vnr.Area** containing missing values. In this case, if there was no area, it should be set to value 0.0. In fact, there are observations with 0.0 and, therefore, we do not know what happened to the 4 observations with NAs for this variable. We decided to remove the 4 observations.
```{r}
summary(ames_train$Mas.Vnr.Area)

ames_train <- na.omit(ames_train)

vis_dat(ames_train) + theme(axis.text.x = element_text(angle = 90, size = 6))+ labs(title = "(C) Data without Missing Values", caption = "Ordered by Type.")

```

Figure 2 (C) shows our final train data set. In total, we have reduced our data from 1,000 to 834 by filtering the Normal Sale Condition and from 834 to 830 by removing observations with NAs in the variable Mas.Vnr.Area.

### **Figure 3**    
**Relatioship between variables**

Here we want to analyse the relationship between variables/factors.

First, we found out that the variable Utilities contains only one value in all observations; then, we found out that the variable Low.Qual.Fin.SF has abdormal distribution; hence, we removed these variables. By running a Linear Regression with all variables, we found out that many variables resulted in extremely large p-values. We removed these variables too.

```{r graph3}
ames_train <- dplyr::select(ames_train, -Utilities, -Low.Qual.Fin.SF, -Lot.Shape, -Land.Contour, -Lot.Config, -House.Style, -Roof.Style, -Mas.Vnr.Type, -Electrical, -Full.Bath, -Half.Bath, -Bedroom.AbvGr, -TotRms.AbvGrd, -Paved.Drive)
```

We compared many variables to see the relationship between them. Here we show some of the relationships among numerical variables that we considered important and that had small p-values in the full linear model created in the step before. 

```{r}
#ggpairs(dplyr::select_if(ames_train, is.numeric))
train_middle <- dplyr::select(ames_train, price, area, Lot.Area, Overall.Qual, Overall.Cond, Year.Built, Year.Remod.Add, Total.Bsmt.SF, Fireplaces, Open.Porch.SF)
ggpairs(train_middle, progress = FALSE,  
        upper = list(
            continuous = wrap("cor", size = 3, alignPercent = 1)
        )) + theme(text = element_text(size = 6), axis.text.y= element_text(angle = 0, size = 6) , axis.text.x = element_text(angle = 90, size = 6))+ labs(title = "Variables relationship")
# 
# 
# library(corrplot)
# res <- cor(train_middle)
# 
# corrplot(res, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45) + 
#   labs(title = "Correlation between numerical variables with small p-value")
```

As we can see in Figure 3, **the numerical variables do not correlate much between each other**, except with price. **Price is highly correlated with area, Overall.Qual and Total.Bsmt.SF**. Overall.Qual is somehow slightly correlated to Year.Built and to Year.Remod.Add, which makes sense. We will keep this information in mind to possibly decide to remove some of those. We investigate more about Colinearity in further analysis.





* * *

## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis

### Section 2.1 An Initial Model
In building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is **not** to identify the "best" possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.

Based on your EDA, select *at most* 10 predictor variables from “ames_train” and create a linear model for `price` (or a transformed version of price) using those variables. Provide the *R code* and the *summary output table* for your model, a *brief justification* for the variables you have chosen, and a *brief discussion* of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).

* * *

To continue with our analysis, we analysed the relationship of variables with Price to check if they look linear. We tested them by log-transforming both the dependent and independent variables, only the dependent and only the independent. In fact, by log-transforming Price, we ended up in general with better plots, that looked more linear and resulted in higher correlation. Therefore, we will use log(price) in our initial model.
```{r}
ggplot(data = ames_train, aes(x=price, y=Overall.Qual))  + geom_point() + stat_smooth(method = "lm", col = "red") + ggtitle("Price x Overall.Qual")
ggplot(data = ames_train, aes(x=log(price), y=Overall.Qual))  + geom_point() + stat_smooth(method = "lm", col = "red") + ggtitle("Log(Price) x Overall.Qual")

cor(ames_train$price, ames_train$Overall.Qual)
cor(log(ames_train$price), ames_train$Overall.Qual)
```

We chose the initial dependent variables based on facts like *not having a large p-value* in the full model (all variables included) tested before, on **having high correlation** with the dependent variable or based on our experience when trying to find a house to rent.

In general, variables linked to Size, Place, Quality and Age are very intuitive when trying to define the price of a house. Therefore, we included:
 1. area (Size)
 2. Lot.Area (Size)
 3. Total.Bsmt.SF (Size)
 4. Neighborhood (Place)
 5. Land.Slope (Place)
 6. Overall.Cond (Quality)
 7. Overall.Qual (Quality)
 8. Kitchen.Qual (Quality)
 9. Year.Built (Age)
 10. Year.Remod.Add (Age)
 
 As we saw in the EDA, **the price is highly correlated to variables that indicate Area and Quality** as well. We also saw that the **price varies among neighborhoods**. The **age variables are clearly related to the price, once new houses are more expensive than old ones**.
 
We created the Initial Model with the 10 variables and analysed the significance of each factor. We concluded that **many neighborhoods do not have significant impact in the price** and **somehow caused MULTICOLINEARITY in the model, suggesting to remove the variable Year.Built**. This analysis was performed using the **generalized variance inflation** used to check for multicolinearity.
```{r fit_model}
#fit <- lm(log(price) ~ area + MS.Zoning + Lot.Area + Bldg.Type + Neighborhood + Overall.Qual + Overall.Cond + Land.Slope + Condition.1 + Condition.2 + Year.Built + Year.Remod.Add + Roof.Matl + Total.Bsmt.SF + Heating + Central.Air + Bsmt.Full.Bath + Kitchen.Qual + Fireplaces + Open.Porch.SF + Screen.Porch, ames_train)
ames_train$Neighborhood_simplified <- as.character(ames_train$Neighborhood)
ames_train$Neighborhood_simplified[!(ames_train$Neighborhood %in% c("BrDale", "Crawfor", "Edwards", "Greens", "GrnHill", "IDOTRR", "MeadowV", "NWAmes", "OldTown", "SawyerW"))] <- "LowImpact"
ames_train$Neighborhood_simplified <- as.factor(ames_train$Neighborhood_simplified)

ames_train$Neighborhood_very_simplified <- as.character(ames_train$Neighborhood)
ames_train$Neighborhood_very_simplified[!(ames_train$Neighborhood %in% c("BrDale", "Crawfor", "Greens", "GrnHill"))] <- "LowImpact"
ames_train$Neighborhood_very_simplified <- as.factor(ames_train$Neighborhood_very_simplified)

fit_init1 <- lm(log(price) ~ area + log(Lot.Area) + Neighborhood + Overall.Qual + Overall.Cond + Land.Slope + Year.Built + Year.Remod.Add + Kitchen.Qual + Total.Bsmt.SF, ames_train)

fit_init2 <- lm(log(price) ~ area + log(Lot.Area) + Neighborhood_simplified + Overall.Qual + Overall.Cond + Land.Slope + Year.Built + Year.Remod.Add + Kitchen.Qual + Total.Bsmt.SF, ames_train)

fit_init3 <- lm(log(price) ~ area + log(Lot.Area) + Neighborhood_very_simplified + Overall.Qual + Overall.Cond + Land.Slope + Year.Built + Year.Remod.Add + Kitchen.Qual + Total.Bsmt.SF, ames_train)

data.frame(AllNeighbors=summary(fit_init1)$adj.r.squared, Simplified=summary(fit_init2)$adj.r.squared, VerySimplified=summary(fit_init3)$adj.r.squared)

```
With the **table above**, we can see that using all Neighbors or using other two versions of simplification resulted in very similar **Adjusted R Squared**. The simplification was created by removing non-significant Neighborhoods from the models. With this procedure, we end up with a model much more parsimoneous and with less non-significant factors. In future Model Selection, we may test both approaches: simplifying and not simplifying the variable Neighborhood.

In the table bellow we can see that in model fit_init1 the **GVIF^(1/(2*Df))** is > 2.56, suggesting that Year.Built should be removed from the model (GVIF^(1/(2*Df)) > 2 is a common threshold).
On the other hand, for model fit_init3 (very simplified), all the **GVIF^(1/(2*Df))** values are < 1.62.
```{r, message=FALSE}
library(car)
car::vif(fit_init1)
car::vif(fit_init3)
```

Since Year.Built is an attribute that we consider of great importance to define the price of a house, we will continue with the model **fit_init3** as our **Initial Model**.


With the following plots, we checked the **linearity** by checking that the values in the Residuals vs Fitted plot are not very large and the **homoscedasticity** by checking that the residuals are equally spread around the y=0 line and that we barely see a patern in the plot Scale-Location. The **normality** also holds once the Normal Q-Q plot is close to a 45º line.
```{r, message=FALSE, warning=FALSE}
fit_init <- fit_init3
par(mfrow = c(2, 2))
plot(fit_init, which=1:4)
```

The Cook's distance plot revealed that **observations number 209, 610 and 756 should be investigated** since they are much larger than the others. We removed the observation 610 from the train set and created a new model fit_init4 to be our final model fit_init. The reason is justified by the fact that in all plots it is coming out as a strange point, confirming the hyphotesis of the Cook's distance plot. The result was a **much better Normal Q-Q plot**, more close to 45º. **The adjusted R Squared also had a small improvement**.

```{r, message=FALSE, warning=FALSE}
ames_train <- ames_train[-c(610),] #, 422, 62, 209, 756
fit_init4 <- lm(log(price) ~ area + log(Lot.Area) + Neighborhood_very_simplified + Overall.Qual + Overall.Cond + Land.Slope + Year.Built + Year.Remod.Add + Kitchen.Qual + Total.Bsmt.SF, ames_train)

data.frame(
  AllNeighbors=summary(fit_init1)$adj.r.squared, 
  Simplified=summary(fit_init2)$adj.r.squared, 
  VerySimplified=summary(fit_init3)$adj.r.squared,
  VerySimplifiedObsRemoved=summary(fit_init4)$adj.r.squared
  )
par(mfrow = c(2, 2))
plot(fit_init4, which=1:4)

fit_init <- fit_init4


```
```{r} 
summary(fit_init)
```

By the variables' coeficients from the model and p-values, we can see that two factors that influentiate a lot are the ones that tell us if the houses are in the GrnHill and in the Greens neighborhoods; followed by Kitchen quality (FA and TA) and Overall Quality; all of them with a small p-value (p < 0.01).

Bellow is the table indicating the % the price is increased/decreased on average when the house is in one of those two neighborhoods, has Ta or Fa as Kitchen Quality, and the percentage the price increases by unit of Overall Quality (when the respective variable is varying and the others are hold constant).
```{r}
# Increase when house is in GrnHill
a1 <- round((exp(0.5118732876)-1)*100, digits = 1)

# Increase when house is in Greens
a2 <- round((exp(0.2459600565)-1)*100, digits = 1)

# Increase for Kitchen QualityFa
a3 <- round((exp(-0.1429046757)-1)*100, digits = 1)

# Increase for Kitchen QualityTA
a4 <- round((exp(-0.1010095452)-1)*100, digits = 1)

# Increase for Overall.Qual
a5 <- round((exp(0.0800670733)-1)*100, digits = 1)

data.frame(GrnHill=a1,
           Greens=a2,
           QualityFa=a3,
           QualityTa=a4,
           Qual=a5)

```


* * *

### Section 2.2 Model Selection

Now either using `BAS` another stepwise selection procedure choose the "best" model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?

* * *
Using BAS with HPM and BPM, and stepAIC (k=log(n), for stepBIC), we obtained 3 models where the only difference to the initial model is that they had the two variables related to the **Land.Slope removed**. The **three models are the same** and have RMSE equals to 19737.34. This means that this final model is the best one under more than one perspective/approach and, therefore, one could be more confident to choose it.

```{r model_select}
model.stepBIC <- stepAIC(fit_init, k = log(nrow(ames_train)), trace = FALSE)
pred.train.stepBIC <- predict(model.stepBIC, newdata = ames_train)
pred.stepBIC.rmse <- sqrt(mean((exp(pred.train.stepBIC) - ames_train$price)^2))
pred.stepBIC.rmse
names(model.stepBIC$coefficients)

model.bas <- bas.lm(log(price) ~ area + log(Lot.Area) + Neighborhood_very_simplified + Overall.Qual + Overall.Cond + Land.Slope + Year.Built + Year.Remod.Add + Kitchen.Qual + Total.Bsmt.SF,
                 data = ames_train, prior = "BIC", modelprior=uniform())

pred.train.HPM <- predict(model.bas, newdata = ames_train, estimator="HPM")
pred.HPM.rmse <- sqrt(mean((exp(pred.train.HPM$fit) - ames_train$price)^2))
pred.HPM.rmse
pred.train.HPM$best.vars

pred.train.BPM <- predict(model.bas, newdata = ames_train, estimator="BPM")
pred.BPM.rmse <- sqrt(mean((exp(pred.train.BPM$fit) - ames_train$price)^2))
pred.BPM.rmse
pred.train.BPM$best.vars

```

```{r, warning=FALSE}
pred.train.init <- predict(fit_init, newdata = ames_train)
pred.init.rmse <- sqrt(mean((exp(pred.train.init) - ames_train$price)^2))
pred.init.rmse


fit_selection <- lm(log(price) ~ area + log(Lot.Area) + Neighborhood_very_simplified + 
    Overall.Qual + Overall.Cond + Year.Built + Year.Remod.Add + 
    Kitchen.Qual + Total.Bsmt.SF, data = ames_train)
pred.train.selection <- predict(fit_selection, newdata = ames_train)
pred.selection.rmse <- sqrt(mean((exp(pred.train.selection) - ames_train$price)^2))
```


* * *

### Section 2.3 Initial Model Residuals
One way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.

* * *

The residuals plots of the Initial Model (fit_init) and the selected model (fit_selection) are very similar. They do not look to follow any patter and look randomly spread around y=0 line. The residuals values are not too large, being mostly between -0.2 and 0.2, while there are only two observations very close to 0.4 (large residual). Most of observations have residuals close to zero,  and our models have good Adjusted R Squared of around 0.92, being our selected model the one with the highest Adjusted R2. When we plot the predicted values versus the truth values or prices, we see how residuals behave along the diagonal line and again that both models are very similar.

```{r model_resid} 

par(mfrow = c(2,2))
plot(fit_init, which=1)
plot(fit_selection, which=1)
plot(ames_train$price, exp(predict(fit_init)))
plot(ames_train$price, exp(predict(fit_selection)))

data.frame(Model=c("Initial", "Selected"),
           Adj.R.Squared= c(summary(fit_selection)$adj.r.squared, summary(fit_init)$adj.r.squared))
```

* * *

### Section 2.4 Initial Model RMSE

You can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).

* * *
We computed the RMSE before and here we compare both for initial and selected model:

```{r model_rmse}

data.frame(Model=c("Initial", "Selected"), 
           RMSE=c(pred.init.rmse, pred.BPM.rmse)
           )
```

* * *

We can see that the RMSE is bigger in the Selected model than it was in the Initial model. On the other hand, the Adjusted R2 is bigger in the Selected model than it is in the initial model. Therefore, it is difficult do decide which model we should pick as our final model. Following a parsimonious approach, we would choose the model that removes the variables associated to the Lot.Slope.


### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.

```{r loadtest, message = FALSE}
load("ames_test.Rdata")
```

Use your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?

* * *
By computing the RMSE with the test data, we can see that it does not differs much from the one computed using the trian data. Therefore, we can say that our model is performing well to predict the price of houses.


```{r initmodel_test}

# Create variable Neighborhood_very_simplified
ames_test$Neighborhood_very_simplified <- as.character(ames_test$Neighborhood)
ames_test$Neighborhood_very_simplified[!(ames_test$Neighborhood %in% c("BrDale", "Crawfor", "Greens", "GrnHill"))] <- "LowImpact"
ames_test$Neighborhood_very_simplified <- as.factor(ames_test$Neighborhood_very_simplified)


pred.test.selection <- predict(fit_selection, newdata = ames_test)
pred.selection.rmse.test <- sqrt(mean((exp(pred.test.selection) - ames_test$price)^2))

data.frame(Train_RMSE=pred.selection.rmse, Test_RMSE=pred.selection.rmse.test)

```

* * *


## Part 3 Development of a Final Model

Now that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.  

Carefully document the process that you used to come up with your final model, so that you can answer the questions below.

### Section 3.1 Final Model

Provide the summary table for your model.

* * *

We are going back and apply stepAIC (AIC and BIC) and BAS method to select our final Model, comparing the results with the initial models models selected in the previows section. We are going to create a summary for RMSE and, after picking our best option, we will plot out summary table for that model. We will **keep our transformations** done in previows analysis and consider only part of the variables, the ones we considered good options in previows analysis.

```{r model_playground}
model_1 <- lm(log(price) ~ area + MS.Zoning + log(Lot.Area) + Bldg.Type + Neighborhood_very_simplified + Overall.Qual + Overall.Cond + Land.Slope + Condition.1 + Condition.2 + Year.Built + Year.Remod.Add + Roof.Matl + Total.Bsmt.SF + Heating + Central.Air + Bsmt.Full.Bath + Kitchen.Qual + Fireplaces + Open.Porch.SF + Screen.Porch, ames_train)
summary(model_1)

model.stepAIC <- stepAIC(model_1, k=2, trace = FALSE)
model.stepBIC <- stepAIC(model_1, k=log(nrow(ames_train)), trace=FALSE)

summary(model.stepBIC)

pred.many.train.stepAIC <- predict(model.stepAIC, newdata = ames_train)
pred.many.stepAIC.rmse <- sqrt(mean((exp(pred.many.train.stepAIC) - ames_train$price)^2))

pred.many.train.stepBIC <- predict(model.stepBIC, newdata = ames_train)
pred.many.stepBIC.rmse <- sqrt(mean((exp(pred.many.train.stepBIC) - ames_train$price)^2))

pred.many.test.stepAIC <- predict(model.stepAIC, newdata = ames_test)
pred.many.stepAIC.rmse.test <- sqrt(mean((exp(pred.many.test.stepAIC) - ames_test$price)^2))

pred.many.test.stepBIC <- predict(model.stepBIC, newdata = ames_test)
pred.many.stepBIC.rmse.test <- sqrt(mean((exp(pred.many.test.stepBIC) - ames_test$price)^2))

data.frame(Model=c("Initially Selected", "stepAIC", "stepBIC"),
            Adj_R2=c(summary(fit_selection)$adj.r.squared,summary(model.stepAIC)$adj.r.squared,summary(model.stepBIC)$adj.r.squared),
            AIC=c(AIC(fit_selection), AIC(model.stepAIC), AIC(model.stepBIC)),
            BIC=c(BIC(fit_selection), BIC(model.stepAIC), BIC(model.stepBIC)),
            RMSE_train=c(pred.selection.rmse, pred.many.stepAIC.rmse, pred.many.stepBIC.rmse),
            RMSE_test=c(pred.selection.rmse.test, pred.many.stepAIC.rmse.test, pred.many.stepBIC.rmse.test)
 )
```

The model selected by stepAIC is our best model. It has bigger Adjusted R2 than our selected initial model and smaller RMSE in both train and test cases. The only con of this model is that comparing the train and test RMSE, this is the one in which we see the biggest difference. Despite that, the values themselves are still smaller than the ones found by the Selected initial model and stepBIC model.

The summary of our final model is:
```{r}
final_model <- model.stepAIC
summary(final_model)
```
We can compare this model with our initial model, to see the differences in the residuals plots:
```{r}
par(mfrow = c(2,2))
plot(final_model, which=1)
plot(fit_selection, which=1)
plot(ames_train$price, exp(predict(final_model)))
plot(ames_train$price, exp(predict(fit_selection)))
```

The plots are very similar, with residuals randomly distributed along the y=0 line. When analysing the predicted vs. true value of price, we can see that for our final model the values are closer to the diagonal, which is consistent with the RMSE results (that are smaller in our final model than in our initial model).

* * *

### Section 3.2 Transformation

Did you decide to transform any variables?  Why or why not? Explain in a few sentences.

* * *

Yes, we have **log-transformed** the **price** and **Lot.Area**. The **reasons why** were already described in the beggining of this report. In summary, we obtained **better Adjusted R Squared** when log-transforming these variables.

```{r model_assess}

```

* * *

### Section 3.3 Variable Interaction

Did you decide to include any variable interactions? Why or why not? Explain in a few sentences.

* * *
We did not included variable interaction in the sense of, for instance, multiplying two factors. On the other hand, we have simplified one categorical variable (Neighborhood), in a way that, for the model, we transformed N binary variables into 1 binary variable named LowImpact. This "LowImpact" value is part of the new variable **Neighborhood_very_simplified** which was also explained before in this report. This simplification could be considered as interaction, since it is an operation of **OR** between the binary variables in the model (which are created considering the values of Neighborhood).

```{r model_inter}
```

* * *

### Section 3.4 Variable Selection

What method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.

* * *

In this report we have performed model selection in many ways, forming initial models with a few variables and a model with more variables. We have chosen the stepAIC and stepBIC to select the best model acording to AIC and BIC and compared their Adjusted R Squared and Residuals.

I have selected these methods because they remove the variables until you have no improvement in the model. That means that we will start with a big model and, therefore, a tendency to select more variables than in our Initial Model. In fact, that was excactly what happened.

```{r model_select}
```

* * *

### Section 3.5 Model Testing

How did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.

* * *
The out-of-sample data was used to assess if the errors found with the train data were too distant from the errors found with the new data. We did not find a big difference in the residual plots of training and testing observations in our models. The test set confirmed that our final model is performing well on the training and testing sets.

```{r model_testing}
```

* * *

## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

For your final model, create and briefly interpret an informative plot of the residuals.

* * *

As shown and commented before, this is the Residuals vs Fitted plot. We can see that the residual values are not big and that they are spread around the y=0 line.

```{r}
plot(final_model, which=1)
```



* * *

### Section 4.2 Final Model RMSE

For your final model, calculate and briefly comment on the RMSE.

* * *

As computed and shown before, the RMSE of the final model is the smaller one that we compared.
The small RMSE can be explained by the small differences between the true price and the predicted price, as ploted bellow:
```{r}
plot(ames_train$price, exp(predict(final_model)))
```



* * *

### Section 4.3 Final Model Evaluation

What are some strengths and weaknesses of your model?



* * *
We can say that some **strenghts** are that our factors are statistically significant to predict price, that our model has a good adjusted R2 and small residuals, as explained above. This means that we are likelly to be able to predict the price of houses and say if the current sale price is bellow the expected price. Among the **weaknesses** we could say that our model perform the worse the bigger is the price. Since we could have bigger profit selling the most expensives houses. So, our model can limit our coverage to low to medium priced houses. Our model also may give us the wrong expected price for the highest priced houses and even puts us in the opposite situation of having profit. Therefore, our model could be more appropriate for houses that are not so expensive, or, if it is the case of an expensive house, its current selling price should be much bellow the expected price, so that we would reduce the risks of a loss.

* * *

### Section 4.4 Final Model Validation

Testing your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. 

You will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention:
* What is the RMSE of your final model when applied to the validation data?  
* How does this value compare to that of the training data and/or testing data?
* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?
* From this result, does your final model properly reflect uncertainty?

```{r loadvalidation, message = FALSE}
load("ames_validation.Rdata")
```

* * *

We first create our Neighborhood_very_simplified variable and remove observations with new factor levels. These were found in variables MS_Zoning (1 observation) and Heating (1 observation), dropping from 763 to 761 observations in the validation set. We also dropped one more observation (#490), because it has value NA for variable Bsmt.Full.Bath.

```{r model_validate}

ames_validation$Neighborhood_very_simplified <- as.character(ames_validation$Neighborhood)
ames_validation$Neighborhood_very_simplified[!(ames_validation$Neighborhood %in% c("BrDale", "Crawfor", "Greens", "GrnHill"))] <- "LowImpact"
ames_validation$Neighborhood_very_simplified <- as.factor(ames_validation$Neighborhood_very_simplified)
levels(ames_validation$MS.Zoning)

nrow(ames_validation)
ames_validation <- ames_validation[ames_validation$MS.Zoning!="A (agr)",]
nrow(ames_validation)
ames_validation <- ames_validation[ames_validation$Heating!="Floor",]
nrow(ames_validation)
ames_validation <- ames_validation[-490,]

summary(ames_validation$price)
pred.final.val <- predict(final_model, newdata = ames_validation)
pred.final.rmse.val <- sqrt(mean((exp(pred.final.val) - ames_validation$price)^2))
data.frame(Model=c("Final Model"),
            RMSE_train=c(pred.selection.rmse),
            RMSE_test=c(pred.selection.rmse.test),
            RMSE_validation=c(pred.final.rmse.val)
 )

# Get dataset of predictions and confidence intervals
pred <- predict(final_model, ames_validation, interval="confidence", se.fit=TRUE, level = 0.95)
pred <- data.frame(pred)

pred$price <- ames_validation$price
pred$fit.lwr <- exp(pred$fit.lwr)
pred$fit.upr <- exp(pred$fit.upr)
# Get Coverage
pred.val <- pred %>% summarize(cover = sum(price >= fit.lwr & price <= fit.upr)/n())
pred.val

```

Our final model obtained a better value for the **RMSE** in the valitation set than in the test set. Both test and validation sets seems to have a RMSE around the one obtained using the train set, which is consistent with what we could expect from a good model.

On the other hand, only **30.5%** of the 95% **predictive confidence intervals** contained the true price of the validation observations. Therefore, our model **is not reflecting uncertainty**.

* * *

## Part 5 Conclusion

Provide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. 

* * *

NOTE: Write your written response to part 5 here. Delete this note before you submit your work.

Conclusion (10 points): must include a summary of results and a discussion of things learned

* * *
